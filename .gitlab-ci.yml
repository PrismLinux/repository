default:
  image: archlinux:latest
  before_script:
    - pacman -Syu --noconfirm
    - pacman -S --noconfirm curl jq base-devel git unzip rsync

# ==============================================================================
# GLOBAL VARIABLES
# ==============================================================================

variables:
  # --- Repository Configuration ---
  REPO_NAME: "prismlinux"
  ARCH: "x86_64"

  # --- Directory and File Configuration ---
  REPO_DIR: "public" # This is the directory GitLab Pages deploys
  CACHE_DIR: "cache"

  # --- State and Trigger Files ---
  CHECKSUMS_FILE: "package_checksums.txt"
  REMOTE_PACKAGES_FILE: "remote_packages.json"
  REBUILD_TRIGGER_FILE: "rebuild_packages.txt"
  PACKAGES_CONFIG: "packages.txt"

# ==============================================================================
# WORKFLOW RULES
# ==============================================================================

# Centralized rules to control when the entire pipeline runs
workflow:
  rules:
    - if: $CI_PIPELINE_SOURCE == 'schedule'
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
    - if: $CI_PIPELINE_SOURCE == 'merge_request_event'
    - if: $CI_PIPELINE_SOURCE == 'web'
    - if: $FORCE_REBUILD == "true"

# ==============================================================================
# PIPELINE STAGES
# ==============================================================================

stages:
  - validate
  - check
  - build
  - deploy
  - cleanup

# ==============================================================================
# CACHE CONFIGURATION
# ==============================================================================

cache:
  key: "repo-cache-${CI_COMMIT_REF_SLUG}"
  paths:
    - "${CACHE_DIR}/"
    - "${CHECKSUMS_FILE}"
    - "${REBUILD_TRIGGER_FILE}"
    - "${REMOTE_PACKAGES_FILE}"
  policy: pull-push

# ==============================================================================
# STAGE: VALIDATE
# ==============================================================================

validate_config:
  stage: validate
  script:
    - |
      echo "Validating configuration..."

      # Check if packages config file exists
      if [ ! -f "${PACKAGES_CONFIG}" ]; then
        echo "ERROR: Package configuration file '${PACKAGES_CONFIG}' not found!"
        exit 1
      fi

      # Check if GitLab token is set
      if [ -z "$GITLAB_TOKEN" ]; then
        echo "ERROR: CI/CD variable 'GITLAB_TOKEN' is not set! It's required to read project releases."
        exit 1
      fi

      # Validate packages config format
      echo "Checking '${PACKAGES_CONFIG}' format..."
      while IFS= read -r line || [[ -n "$line" ]]; do
        # Skip comments and empty lines
        if [[ "$line" =~ ^[[:space:]]*# ]] || [[ -z "$line" ]]; then
          continue
        fi
        
        project_id=$(echo "$line" | awk '{print $1}')
        if ! [[ "$project_id" =~ ^[0-9]+$ ]]; then
          echo "ERROR: Invalid format in '${PACKAGES_CONFIG}'. Line should contain a numeric Project ID: '$line'"
          exit 1
        fi
        echo "  ✓ Valid Project ID: $project_id"
      done < "${PACKAGES_CONFIG}"

      echo "Configuration validation passed!"

# ==============================================================================
# STAGE: CHECK
# ==============================================================================

check_package_updates:
  stage: check
  script:
    - |
      set -eo pipefail
      echo "=== Checking for package updates ==="

      mkdir -p "${CACHE_DIR}"

      # Initialize checksum arrays
      declare -A old_checksums
      declare -A new_checksums

      # Load existing checksums
      if [ -f "${CHECKSUMS_FILE}" ]; then
        echo "Loading existing checksums from '${CHECKSUMS_FILE}'..."
        while IFS='|' read -r filename checksum; do
          [[ -n "$filename" && -n "$checksum" ]] && old_checksums["$filename"]="$checksum"
        done < "${CHECKSUMS_FILE}"
      else
        echo "No existing checksums file found. All packages will be treated as new."
      fi

      # Get project IDs from config
      PROJECT_IDS=$(grep -vE '^\s*#|^\s*$' "${PACKAGES_CONFIG}" | awk '{print $1}' | tr '\n' ' ')
      echo "Processing Project IDs: ${PROJECT_IDS}"

      # Fetch remote packages from GitLab releases
      remote_packages_json=$( \
        for project_id in ${PROJECT_IDS}; do
          echo "Fetching releases for project ${project_id}..." >&2
          releases_url="https://gitlab.com/api/v4/projects/${project_id}/releases"
          
          latest_release=$(curl --silent --show-error --fail \
            --header "PRIVATE-TOKEN: $GITLAB_TOKEN" \
            "${releases_url}" | jq -r '.[0] // empty')
          
          if [ -z "$latest_release" ]; then
            echo "  -> No releases found for project ${project_id}, skipping." >&2
            continue
          fi
          
          echo "${latest_release}" | jq -c '.assets.links[]? | select(.name | endswith(".pkg.tar.zst")) | {(.name): .url}'
        done | jq -s 'add' \
      )

      # Handle case where no packages are found
      if [ -z "$remote_packages_json" ] || [ "$remote_packages_json" == "null" ]; then
        echo "No remote packages found across all projects. Nothing to do."
        echo "{}" > "${REMOTE_PACKAGES_FILE}"
        echo "REBUILD_NEEDED=false" > build.env
        exit 0
      fi

      echo "${remote_packages_json}" > "${REMOTE_PACKAGES_FILE}"
      echo "Found a total of $(echo "$remote_packages_json" | jq 'length') remote packages."

      # Verify package checksums
      echo "=== Verifying package checksums ==="
      changed_packages=()

      echo "$remote_packages_json" | jq -r 'to_entries[] | "\(.key)|\(.value)"' | while IFS='|' read -r pkg_filename download_url; do
        echo "  Checking: ${pkg_filename}"
        temp_file=$(mktemp)
        
        if ! curl --location --silent --show-error --fail \
          --header "PRIVATE-TOKEN: $GITLAB_TOKEN" \
          --output "$temp_file" "$download_url"; then
          echo "    ✗ ERROR: Failed to download ${pkg_filename} for checksum verification. Skipping."
          rm -f "$temp_file"
          continue
        fi
        
        checksum=$(sha256sum "$temp_file" | awk '{print $1}')
        rm "$temp_file"
        new_checksums["$pkg_filename"]="$checksum"
        
        if [[ ! -v old_checksums["$pkg_filename"] ]] || [[ "${old_checksums[$pkg_filename]}" != "$checksum" ]]; then
          echo "    🔄 CHANGED (new checksum: ${checksum})"
          changed_packages+=("$pkg_filename")
        else
          echo "    ✓ Unchanged"
        fi
      done

      # Determine if rebuild is needed
      if [ ${#changed_packages[@]} -gt 0 ] || [ "$FORCE_REBUILD" == "true" ]; then
        echo "=== Found ${#changed_packages[@]} changed packages. Triggering rebuild. ==="
        printf '%s\n' "${changed_packages[@]}" | tee "${REBUILD_TRIGGER_FILE}"
        echo "REBUILD_NEEDED=true" >> build.env
      else
        echo "=== No package changes detected. ==="
        > "${REBUILD_TRIGGER_FILE}"
        echo "REBUILD_NEEDED=false" >> build.env
      fi

      # Update checksums file
      echo "Updating '${CHECKSUMS_FILE}' with latest checksums..."
      > "${CHECKSUMS_FILE}"
      for pkg_filename in "${!new_checksums[@]}"; do
        echo "${pkg_filename}|${new_checksums[$pkg_filename]}" >> "${CHECKSUMS_FILE}"
      done

  artifacts:
    reports:
      dotenv: build.env
    paths:
      - "${REBUILD_TRIGGER_FILE}"
      - "${CHECKSUMS_FILE}"
      - "${REMOTE_PACKAGES_FILE}"
    expire_in: 1 hour

# ==============================================================================
# STAGE: BUILD
# ==============================================================================

build_repository:
  stage: build
  needs:
    - job: check_package_updates
      artifacts: true
  script:
    - |
      set -eo pipefail
      echo "=== Building Repository ==="

      # Validate remote packages file exists
      if [ ! -f "${REMOTE_PACKAGES_FILE}" ]; then
          echo "ERROR: Remote packages file '${REMOTE_PACKAGES_FILE}' not found. Cannot proceed."
          exit 1
      fi

      # Setup directories
      REPO_ARCH_DIR="${REPO_DIR}/${ARCH}"
      mkdir -p "${REPO_ARCH_DIR}" "${CACHE_DIR}"

      # Load packages that need rebuilding
      declare -A packages_to_rebuild
      if [ -f "${REBUILD_TRIGGER_FILE}" ]; then
        while IFS= read -r pkg_filename; do
          [[ -n "$pkg_filename" ]] && packages_to_rebuild["$pkg_filename"]=1
        done < "${REBUILD_TRIGGER_FILE}"
      fi

      # Download packages
      echo "Reading package list from '${REMOTE_PACKAGES_FILE}'..."
      successful_downloads=0
      failed_downloads=0

      jq -r 'to_entries[] | "\(.key)|\(.value)"' "${REMOTE_PACKAGES_FILE}" | while IFS='|' read -r pkg_filename download_url; do
        cached_file="${CACHE_DIR}/${pkg_filename}"
        repo_file="${REPO_ARCH_DIR}/${pkg_filename}"
        
        if [[ -v packages_to_rebuild["$pkg_filename"] ]] || [ "$FORCE_REBUILD" == "true" ] || [ ! -f "$cached_file" ]; then
          if [[ ! -f "$cached_file" ]]; then
            echo "  ⬇️ Downloading new/missing package: ${pkg_filename}"
          else
            echo "  🔄 Re-downloading changed package: ${pkg_filename}"
          fi
          
          if curl --location --silent --show-error --fail \
            --header "PRIVATE-TOKEN: $GITLAB_TOKEN" \
            --output "$repo_file" "$download_url"; then
            echo "    ✓ Downloaded successfully. Updating cache."
            cp "$repo_file" "$cached_file"
            ((successful_downloads++))
          else
            echo "    ✗ ERROR: Failed to download ${pkg_filename}!"
            ((failed_downloads++))
          fi
        else
          echo "  📋 Using cached version of: ${pkg_filename}"
          cp "$cached_file" "$repo_file"
        fi
      done

      # Clean up obsolete packages
      echo "=== Cleaning up obsolete packages ==="
      declare -A remote_packages_lookup
      jq -r 'keys[]' "${REMOTE_PACKAGES_FILE}" | while IFS= read -r pkg_name; do
        remote_packages_lookup["$pkg_name"]=1
      done

      for cached_file in "${CACHE_DIR}"/*.pkg.tar.zst; do
        [ -f "$cached_file" ] || continue
        cached_filename=$(basename "$cached_file")
        if [[ ! -v remote_packages_lookup["$cached_filename"] ]]; then
          echo "  🗑️ Removing obsolete cached package: ${cached_filename}"
          rm "$cached_file"
        fi
      done

      # Create repository database
      echo "=== Creating repository database in '${REPO_ARCH_DIR}' ==="
      cd "${REPO_ARCH_DIR}"
      rm -f "${REPO_NAME}".db* "${REPO_NAME}".files*

      package_count=$(ls -1 ./*.pkg.tar.zst 2>/dev/null | wc -l)
      if [ "$package_count" -gt 0 ]; then
        echo "Building repository database with ${package_count} packages..."
        repo-add "${REPO_NAME}.db.tar.gz" *.pkg.tar.zst
        ln -sf "${REPO_NAME}.db.tar.gz" "${REPO_NAME}.db"
        ln -sf "${REPO_NAME}.files.tar.gz" "${REPO_NAME}.files"
        echo "✓ Repository database created successfully."
      else
        echo "⚠️ No packages found. Creating an empty repository database."
        touch "${REPO_NAME}.db.tar.gz" "${REPO_NAME}.files.tar.gz"
        ln -sf "${REPO_NAME}.db.tar.gz" "${REPO_NAME}.db"
        ln -sf "${REPO_NAME}.files.tar.gz" "${REPO_NAME}.files"
      fi

      # Build summary
      echo "=== Build Summary ==="
      echo "Successful downloads: $successful_downloads"
      echo "Failed downloads: $failed_downloads"
      echo "Total packages in repo: $package_count"

      if [ $failed_downloads -gt 0 ]; then
        echo "🔥 Build failed due to download errors."
        exit 1
      fi

  artifacts:
    paths:
      - "${REPO_DIR}/"
    expire_in: 1 week
  rules:
    - if: $REBUILD_NEEDED == "true"
    - if: $FORCE_REBUILD == "true"
      when: always
    - if: $CI_PIPELINE_SOURCE == 'web'
      when: always

# ==============================================================================
# STAGE: DEPLOY
# ==============================================================================

deploy_website:
  stage: deploy
  needs:
    - job: build_repository
      optional: true
  before_script:
    - pacman -Syu --noconfirm
    - pacman -S --noconfirm curl jq nodejs npm unzip
    - curl -fsSL https://bun.sh/install | bash
    - export PATH="$HOME/.bun/bin:$PATH"
  script:
    - |
      set -eo pipefail
      echo "=== Preparing Website for Deployment ==="

      REPO_ARCH_DIR="${REPO_DIR}/${ARCH}"
      API_DIR="${REPO_DIR}/api"
      mkdir -p "${API_DIR}"

      # Generate packages API
      echo "Generating API file: packages.json"
      (
        if [ -d "${REPO_ARCH_DIR}" ]; then
          for pkg_file in "${REPO_ARCH_DIR}"/*.pkg.tar.zst; do
            [ -f "$pkg_file" ] || continue
            pkg_info=$(pacman -Qip "$pkg_file")
            jq -cn \
              --arg name "$(echo "$pkg_info" | grep -m1 "^Name" | sed 's/Name\s*:\s*//')" \
              --arg version "$(echo "$pkg_info" | grep -m1 "^Version" | sed 's/Version\s*:\s*//')" \
              --arg desc "$(echo "$pkg_info" | grep -m1 "^Description" | sed 's/Description\s*:\s*//' || echo 'No description available.')" \
              --arg arch "$(echo "$pkg_info" | grep -m1 "^Architecture" | sed 's/Architecture\s*:\s*//')" \
              --arg filename "$(basename "$pkg_file")" \
              --arg size "$(ls -lh "$pkg_file" | awk '{print $5}')" \
              --arg modified "$(date -r "$pkg_file" --iso-8601=seconds)" \
              --arg depends "$(echo "$pkg_info" | grep -m1 "^Depends On" | sed 's/Depends On\s*:\s*//' || echo 'None')" \
              --arg url "$(echo "$pkg_info" | grep -m1 "^URL" | sed 's/URL\s*:\s*//' || echo 'None')" \
              --arg license "$(echo "$pkg_info" | grep -m1 "^Licenses" | sed 's/Licenses\s*:\s*//' || echo 'None')" \
              '{name: $name, version: $version, description: $desc, architecture: $arch, filename: $filename, size: $size, modified: $modified, depends: $depends, url: $url, license: $license}'
          done
        fi
      ) | jq -s . > "${API_DIR}/packages.json"

      # Generate stats API
      echo "Generating API file: stats.json"
      jq -n \
        --argjson total_packages "$(ls -1 "${REPO_ARCH_DIR}"/*.pkg.tar.zst 2>/dev/null | wc -l || echo 0)" \
        --arg repository_size "$(du -sh "${REPO_ARCH_DIR}" 2>/dev/null | awk '{print $1}' || echo '0B')" \
        --arg last_updated "$(date --iso-8601=seconds)" \
        --arg arch "${ARCH}" \
        --arg repo_name "${REPO_NAME}" \
        '{total_packages: $total_packages, repository_size: $repository_size, last_updated: $last_updated, architecture: $arch, repository_name: $repo_name}' > "${API_DIR}/stats.json"
        
      # Build custom website if available
      if [ -d "website" ]; then
        echo "Found 'website' directory. Building custom frontend..."
        cd website
        
        if [ -f "package.json" ]; then
          if command -v bun >/dev/null 2>&1; then
            echo "Using 'bun' to install dependencies and build."
            bun install --frozen-lockfile
            bun run build
          else
            echo "Using 'npm' to install dependencies and build."
            npm ci
            npm run build
          fi
          
          echo "Copying built frontend to '${REPO_DIR}'..."
          rsync -a "dist/." "../${REPO_DIR}/"
        else
          echo "Warning: 'website' directory exists but no 'package.json' found. Skipping build."
        fi
        cd ..
      else
        # Generate default website
        echo "No 'website' directory found. Generating a simple default index.html..."
        cat > "${REPO_DIR}/index.html" << 'HTMLEOF'
      <!DOCTYPE html>
      <html lang="en">
      <head>
          <title>REPO_NAME_PLACEHOLDER Repository</title>
          <meta charset="utf-8">
          <meta name="viewport" content="width=device-width, initial-scale=1">
          <style>
              body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; margin: 2rem; background-color: #f8f9fa; color: #212529; }
              .container { max-width: 800px; margin: 0 auto; background: #fff; padding: 2rem; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,.05); }
              h1, h2 { color: #343a40; border-bottom: 1px solid #dee2e6; padding-bottom: .5rem; }
              pre { background: #e9ecef; padding: 1rem; border-radius: 4px; white-space: pre-wrap; word-break: break-all; }
              code { font-family: "SFMono-Regular", Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; }
              .card { background: #f1f3f5; padding: 1rem; border-radius: 4px; margin-bottom: 1rem; }
              #stats span { font-weight: 700; }
          </style>
      </head>
      <body>
          <div class="container">
              <header>
                  <h1>REPO_NAME_PLACEHOLDER Repository</h1>
              </header>
              <section id="usage">
                  <h2>How to Use</h2>
                  <p>Add the following to your <code>/etc/pacman.conf</code>:</p>
                  <pre><code>[REPO_NAME_PLACEHOLDER]
      Server = PAGES_URL_PLACEHOLDER/ARCH_PLACEHOLDER</code></pre>
              </section>
              <section id="stats">
                  <h2>Statistics</h2>
                  <div class="card">
                      <p>Total Packages: <span id="stat-packages">Loading...</span></p>
                      <p>Repository Size: <span id="stat-size">Loading...</span></p>
                      <p>Last Updated: <span id="stat-updated">Loading...</span></p>
                      <p>Architecture: <span>ARCH_PLACEHOLDER</span></p>
                  </div>
              </section>
              <section id="packages">
                  <h2>Available Packages</h2>
                  <div id="package-list">Loading...</div>
              </section>
          </div>
          <script>
              document.addEventListener("DOMContentLoaded", () => {
                  fetch("/api/stats.json").then(response => response.json()).then(data => {
                      document.getElementById("stat-packages").textContent = data.total_packages;
                      document.getElementById("stat-size").textContent = data.repository_size;
                      document.getElementById("stat-updated").textContent = new Date(data.last_updated).toLocaleString();
                  }).catch(error => console.error("Failed to load stats:", error));
                  
                  fetch("/api/packages.json").then(response => response.json()).then(packages => {
                      const packageList = document.getElementById("package-list");
                      packageList.innerHTML = packages.length === 0 
                          ? '<p>No packages available.</p>'
                          : packages.map(pkg => `<div class="card"><strong>${pkg.name} ${pkg.version}</strong><br><small>${pkg.description || "No description"}</small></div>`).join("");
                  }).catch(error => console.error("Failed to load packages:", error));
              });
          </script>
      </body>
      </html>
      HTMLEOF
        # Replace placeholders with actual values
        sed -i "s/REPO_NAME_PLACEHOLDER/${REPO_NAME}/g" "${REPO_DIR}/index.html"
        sed -i "s/PAGES_URL_PLACEHOLDER/${CI_PAGES_URL}/g" "${REPO_DIR}/index.html"
        sed -i "s/ARCH_PLACEHOLDER/${ARCH}/g" "${REPO_DIR}/index.html"
      fi

      echo "=== Deployment Payload Summary ==="
      ls -R "${REPO_DIR}"

pages:
  stage: deploy
  script:
    - echo "Deploying GitLab Pages..."
  artifacts:
    paths:
      - public
  rules:
    - if: $REBUILD_NEEDED == "true"
    - if: $FORCE_REBUILD == "true"
      when: always
    - if: $CI_PIPELINE_SOURCE == 'web'
      when: always
    - if: $CI_PIPELINE_SOURCE == 'schedule'

# ==============================================================================
# STAGE: CLEANUP
# ==============================================================================

cleanup_old_artifacts:
  stage: cleanup
  script:
    - |
      set -eo pipefail
      echo "=== Cleaning up old job artifacts ==="

      if [ -z "$GITLAB_TOKEN" ]; then
        echo "Warning: GITLAB_TOKEN not set. Cannot perform API-based cleanup."
        exit 0
      fi

      CUTOFF_DATE=$(date -d '7 days ago' --iso-8601)
      echo "Deleting artifacts from successful jobs finished before: $CUTOFF_DATE"

      curl --silent --show-error --fail \
        --header "PRIVATE-TOKEN: $GITLAB_TOKEN" \
        "https://gitlab.com/api/v4/projects/${CI_PROJECT_ID}/jobs?scope[]=success&per_page=100" | \
      jq -r ".[] | select(.finished_at != null and .finished_at < \"$CUTOFF_DATE\" and .artifacts_file != null) | .id" | \
      while IFS= read -r job_id; do
        echo "  -> Deleting artifacts for job #${job_id}"
        curl --request DELETE --silent \
          --header "PRIVATE-TOKEN: $GITLAB_TOKEN" \
          "https://gitlab.com/api/v4/projects/${CI_PROJECT_ID}/jobs/${job_id}/artifacts" || \
          echo "    (Failed to delete, continuing...)"
      done

      echo "Cleanup complete."

  rules:
    - if: $CI_PIPELINE_SOURCE == 'schedule'
    - if: $CI_PIPELINE_SOURCE == 'web'
      when: manual
  allow_failure: true
